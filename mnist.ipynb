{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatGPT\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = nn.Sequential([\n",
    "        nn.Dense(128),\n",
    "        nn.relu,\n",
    "        nn.Dense(128),\n",
    "        nn.relu,\n",
    "        nn.Dense(num_classes),\n",
    "        nn.log_softmax,\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_loss_fn(model):\n",
    "    def loss_fn(params, batch):\n",
    "        inputs, targets = batch\n",
    "        logits = model.apply({'params': params}, inputs)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=targets))\n",
    "        return loss\n",
    "    return loss_fn\n",
    "\n",
    "def create_metrics():\n",
    "    metrics = {\n",
    "        'accuracy': flax.metrics.accuracy,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def train_step(state, batch):\n",
    "    model = state.model\n",
    "    optimizer = state.optimizer\n",
    "    loss_fn = state.loss_fn\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "    inputs, targets = batch\n",
    "    logits, grads = grad_fn(model.params, (inputs, targets))\n",
    "    model = model.apply_gradients(grads=grads)\n",
    "\n",
    "    metrics = state.metrics\n",
    "    metrics = flax.metrics.update_metrics(metrics, logits, targets)\n",
    "\n",
    "    new_state = state.replace(model=model, metrics=metrics)\n",
    "    return new_state\n",
    "\n",
    "def eval_step(state, batch):\n",
    "    model = state.model\n",
    "    loss_fn = state.loss_fn\n",
    "\n",
    "    inputs, targets = batch\n",
    "    logits = model.apply({'params': model.params}, inputs)\n",
    "    loss = loss_fn(model.params, (inputs, targets))\n",
    "    metrics = state.metrics\n",
    "    metrics = flax.metrics.update_metrics(metrics, logits, targets)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Training configuration\n",
    "    num_classes = 10\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Prepare the dataset\n",
    "    train_dataset, test_dataset = load_mnist_dataset()  # Your MNIST dataset loading function\n",
    "\n",
    "    # Create the model\n",
    "    model = create_model(num_classes)\n",
    "    params = model.init(jax.random.PRNGKey(0), jnp.ones([1, 784]))['params']\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate).create(params)\n",
    "\n",
    "    # Create the loss function and metrics\n",
    "    loss_fn = create_loss_fn(model)\n",
    "    metrics = create_metrics()\n",
    "\n",
    "    # Create the initial training state\n",
    "    initial_state = train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_metrics = initial_state.metrics\n",
    "        for batch in train_loader:\n",
    "            initial_state = train_step(initial_state, batch)\n",
    "            train_metrics = flax.metrics.merge_metrics(train_metrics, initial_state.metrics)\n",
    "\n",
    "        # Evaluation\n",
    "        eval_metrics = initial_state.metrics\n",
    "        for batch in test_loader:\n",
    "            eval_metrics = eval_step(initial_state, batch)\n",
    "\n",
    "        # Print epoch summary\n",
    "        train_summary = flax.metrics.to_scalar(train_metrics)\n",
    "        eval_summary = flax.metrics.to_scalar(eval_metrics)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_summary['loss']:.4f} | \"\n",
    "              f\"Train Accuracy: {train_summary['accuracy']:.4f} | \"\n",
    "              f\"Eval Loss: {eval_summary['loss']:.4f} | \"\n",
    "              f\"Eval Accuracy: {eval_summary['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    # Define the paths to the MNIST dataset files\n",
    "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    paths = []\n",
    "    for file in files:\n",
    "        path = os.path.join('mnist', file)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs('mnist', exist_ok=True)\n",
    "            url = base_url + file\n",
    "            print(f\"Downloading {url}...\")\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        paths.append(path)\n",
    "\n",
    "    # Load the training data\n",
    "    with gzip.open(paths[0], 'rb') as f:\n",
    "        x_train = np.frombuffer(f.read(), dtype=np.uint8, offset=16).reshape(-1, 28*28)\n",
    "    with gzip.open(paths[1], 'rb') as f:\n",
    "        y_train = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    # Load the test data\n",
    "    with gzip.open(paths[2], 'rb') as f:\n",
    "        x_test = np.frombuffer(f.read(), dtype=np.uint8, offset=16).reshape(-1, 28*28)\n",
    "    with gzip.open(paths[3], 'rb') as f:\n",
    "        y_test = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    # Normalize pixel values\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train = np.eye(10)[y_train]\n",
    "    y_test = np.eye(10)[y_test]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = list(zip(x_train, y_train))\n",
    "    test_dataset = list(zip(x_test, y_test))\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'flax' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/augustine/code/genomic_bottleneck/mnist.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32m/home/augustine/code/genomic_bottleneck/mnist.ipynb Cell 3\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Prepare the dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m train_dataset, test_dataset \u001b[39m=\u001b[39m load_mnist_dataset()  \u001b[39m# Your MNIST dataset loading function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m train_loader \u001b[39m=\u001b[39m flax\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m test_loader \u001b[39m=\u001b[39m flax\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/augustine/code/genomic_bottleneck/mnist.ipynb#W1sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Create the model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'flax' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
